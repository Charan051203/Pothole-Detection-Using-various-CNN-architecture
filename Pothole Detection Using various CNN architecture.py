# -*- coding: utf-8 -*-
"""CNN mini project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10cf23MQKnAJ40F45wZxm_KrTmIhwXKHE
"""

!pip install lime
!pip install shap
!pip install scikeras

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import GlobalAveragePooling2D, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16, VGG19, ResNet50
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import classification_report, confusion_matrix
from skimage.segmentation import mark_boundaries
import lime
from lime import lime_image
import shap
import cv2

tf.test.gpu_device_name()

from google.colab import drive
drive.mount('/content/drive')

#Data preprocessing
train_dir = '/content/drive/MyDrive/dataset/train'
test_dir = '/content/drive/MyDrive/dataset/test'
val_dir = '/content/drive/MyDrive/dataset/val'

img_height, img_width = 224, 224
batch_size = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary'
)

def create_vgg16_model():
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    output = Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inputs=base_model.input, outputs=output)
    return model

def create_vgg19_model():
    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    output = Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inputs=base_model.input, outputs=output)
    return model

def create_resnet50_model():
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    output = Dense(1, activation='sigmoid')(x)
    model = tf.keras.Model(inputs=base_model.input, outputs=output)
    return model

def create_alexnet_model():
    model = Sequential([
        tf.keras.layers.Conv2D(96, 11, strides=4, padding='same', activation='relu', input_shape=(img_height, img_width, 3)),
        tf.keras.layers.MaxPooling2D(3, strides=2),
        tf.keras.layers.Conv2D(256, 5, padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(3, strides=2),
        tf.keras.layers.Conv2D(384, 3, padding='same', activation='relu'),
        tf.keras.layers.Conv2D(384, 3, padding='same', activation='relu'),
        tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu'),
        tf.keras.layers.MaxPooling2D(3, strides=2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(4096, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(4096, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

# Train and evaluate models
models = {
    'VGG16': create_vgg16_model(),
    'VGG19': create_vgg19_model(),
    'ResNet50': create_resnet50_model(),
    'AlexNet': create_alexnet_model()
}

best_model = None
best_accuracy = 0

for name, model in models.items():
    print(f"Training {name} model...")

    model.compile(optimizer=Adam(learning_rate=0.0001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint(f'{name}_best_model.keras', save_best_only=True)

    history = model.fit(
        train_generator,
        steps_per_epoch=train_generator.samples // batch_size,
        epochs=200,
        validation_data=val_generator,
        validation_steps=val_generator.samples // batch_size,
        callbacks=[early_stopping, model_checkpoint]
    )

# Re-evaluate the model
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"{name} Test Accuracy: {test_accuracy:.4f}")
if test_accuracy > best_accuracy:
  best_accuracy = test_accuracy
  best_model = model

print(f"Best model: {name} with accuracy: {best_accuracy:.4f}")

#Model Evaluation
y_pred = best_model.predict(test_generator)
y_pred_classes = (y_pred > 0.5).astype(int)
y_true = test_generator.classes

print("Classification Report:")
print(classification_report(y_true, y_pred_classes))

print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred_classes))

# LIME
def explain_with_lime(model, image):
    explainer = lime_image.LimeImageExplainer()
    explanation = explainer.explain_instance(image, model.predict, top_labels=1, hide_color=0, num_samples=1000)
    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)
    plt.imshow(mark_boundaries(temp, mask))
    plt.title("LIME Explanation")
    plt.show()

# SHAP
def explain_with_shap(model, image):
    model.layers[-1].activation = None
    data = next(iter(train_generator))
    images = data[0][:1000]
    explainer = shap.DeepExplainer(model, images)
    shap_values = explainer.shap_values(images)
    shap.image_plot(shap_values, images)

# Saliency Maps
def explain_with_saliency(model, image):
    with tf.GradientTape() as tape:
        image_tensor = tf.convert_to_tensor(image[np.newaxis, ...], dtype=tf.float32)
        tape.watch(image_tensor)
        predictions = model(image_tensor)

    gradients = tape.gradient(predictions, image_tensor)
    saliency = tf.reduce_max(tf.abs(gradients), axis=-1)

    plt.imshow(saliency[0], cmap='hot')
    plt.title("Saliency Map")
    plt.colorbar()
    plt.show()

# Grad-CAM
def explain_with_gradcam(model, image):
    last_conv_layer = next(layer for layer in reversed(model.layers) if isinstance(layer, tf.keras.layers.Conv2D))
    last_conv_layer_model = Model(model.inputs, last_conv_layer.output)
    classifier_input = Input(shape=last_conv_layer.output.shape[1:])
    x = classifier_input
    for layer in model.layers[model.layers.index(last_conv_layer)+1:]:
        x = layer(x)
    classifier_model = Model(classifier_input, x)

    with tf.GradientTape() as tape:
        last_conv_layer_output = last_conv_layer_model(image[np.newaxis, ...])
        tape.watch(last_conv_layer_output)
        preds = classifier_model(last_conv_layer_output)
        top_pred_index = tf.argmax(preds[0])
        top_class_output = preds[:, top_pred_index]

    grads = tape.gradient(top_class_output, last_conv_layer_output)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    last_conv_layer_output = last_conv_layer_output.numpy()[0]
    pooled_grads = pooled_grads.numpy()
    for i in range(pooled_grads.shape[-1]):
        last_conv_layer_output[:, :, i] *= pooled_grads[i]

    heatmap = np.mean(last_conv_layer_output, axis=-1)
    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)

    img = image.copy()
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed_img = heatmap * 0.4 + img

    plt.imshow(superimposed_img / 255)
    plt.title("Grad-CAM")
    plt.show()

# Apply XAI methods to sample images
sample_images, _ = next(test_generator)
for i in range(1):
    image = sample_images[i]

    plt.imshow(image)
    plt.title("Original Image")
    plt.show()

    explain_with_lime(best_model, image)
    explain_with_shap(best_model, image)
    explain_with_saliency(best_model, image)
    explain_with_gradcam(best_model, image)

# Print classification report
from sklearn.metrics import classification_report
y_true = test_generator.classes
y_pred = best_model.predict(test_generator).round()
print(classification_report(y_true, y_pred, target_names=['Normal', 'Pothole']))